---
title: "Compare Distributions"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---
<!-- author: "Julian McClellan" -->
<!-- date: "2/something/2020" -->
<!-- output:  -->
<!-- html_document: -->
<!-- toc: true -->

```{r setup, include=FALSE}
source("R/functions.R")
source("R/packages.R")
knitr::opts_chunk$set(echo = F, message = F, warning=F)
options(scipen=999) # No scientific notation
```

```{r, create_specifications}
source("R/create_specifications.R")
```

```{r calc_mean_comparison, include=F}
# Inexperienced Control vs. Experienced Control
# Inexperienced Control vs. Inexperienced Treated
# Inexperienced Treated vs. Experienced Control
# Experienced Control vs. Experienced Treated
specMeanTestVars <- tibble(sample1Name = c("Inexperienced Control",
                                        "Inexperienced Control", 
                                        "Inexperienced Treated",
                                        "Experienced Control"),
                        sample2Name = c("Experienced Control",
                                        "Inexperienced Treated",
                                        "Experienced Control",
                                        "Experienced Treated")) %>%
  tidyr::crossing(spec = c(1, 2), vecSize = 1000)

specSigmaTestVars <- specMeanTestVars %>%
  mutate(outcomeVar = "normsigma_lnorm")

specMeanResults <- specMeanTestVars %>%
  pmap_dfr(.f = compareDistributions, masterDf = specBothFull, mode = "means")
specSigmaResults <- specSigmaTestVars %>%
  pmap_dfr(.f = compareDistributions, masterDf = specBothFull, mode = "means")
```

# Mean Results

Using the [specifications detailed here](https://docs.google.com/spreadsheets/d/1JS-5_iv-WrfzifFsr_1CW5a12G5DfMAGYSs6r4ACfxs/edit?usp=sharing), I took groups (e.g. Experienced Control vs. Experienced Treated) and sampled with replacement to get both groups to have a size of 1000 and then ran Anderson-Darling tests using the kSamples package using the specified `outcomeVar`.

Sizes of each group are detailed with `sample1N` and `sample2N` respectively, along with the asymptotic p-value, names of the groups compared, (Standardized) Anderson-Darling criterion, `AD` (`T.AD`). Sample sizes are often slightly below 1000 due to invalid values.

```{r display_mean_comp_results}
library(kableExtra)

knitr::kable(specMeanResults, "html", digits = 5, caption = "Lognormual Mu Comparison") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F)

knitr::kable(specSigmaResults, "html", digits = 5, caption = "Lognormal Sigma Comparison") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F)
```

```{r calc_draws_comparison}
specDrawsTestVars <- specMeanTestVars %>%
  tidyr::crossing(draws = c(100)) #, 1000, 10000)) # Jesus this takes so long

drawResultsPath <- "data/specDrawsResults_100.rds"
if (!file.exists(drawResultsPath)){
  specDrawsResults <- specDrawsTestVars %>%
    pmap_dfr(.f = compareDistributions, masterDf = specBothFull, mode = "draws")
  saveRDS(specDrawsResults, drawResultsPath)
} else {
  specDrawsResults <- readRDS(drawResultsPath)
}
```

# Draws Results

Again, using the [specifications detailed here](https://docs.google.com/spreadsheets/d/1JS-5_iv-WrfzifFsr_1CW5a12G5DfMAGYSs6r4ACfxs/edit?usp=sharing), for these draw results, I separated groups and using the `normmu_lnorm` and `normsigma_lnorm` values for each observation within those groups, I simulated 100 draws from a log-normal distribution using those values. Thus, each group of sample size $N$ has $N * 100$ random draws from $N$ different log-normal distributions, with some values invalid values being removed.

```{r display_draws_comparison}
knitr::kable(specDrawsResults, "html", digits = 5) %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F)
```
